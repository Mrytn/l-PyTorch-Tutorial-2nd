{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量相关函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., -1.],\n",
      "        [ 1., -1.]]) torch.float32\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32) torch.int32\n"
     ]
    }
   ],
   "source": [
    "l = [[1., -1.], [1., -1.]]\n",
    "t_from_list = torch.tensor(l)\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t_from_array = torch.tensor(arr)\n",
    "print(t_from_list, t_from_list.dtype)\n",
    "print(t_from_array, t_from_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.from_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array:  [[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor :  tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "\n",
      "修改arr\n",
      "numpy array:  [[0 2 3]\n",
      " [4 5 6]]\n",
      "tensor :  tensor([[0, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "\n",
      "修改tensor\n",
      "numpy array:  [[-1  2  3]\n",
      " [ 4  5  6]]\n",
      "tensor :  tensor([[-1,  2,  3],\n",
      "        [ 4,  5,  6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t_from_numpy = torch.from_numpy(arr)\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)\n",
    "print(\"\\n修改arr\")\n",
    "arr[0, 0] = 0\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)\n",
    "print(\"\\n修改tensor\")\n",
    "t_from_numpy[0, 0] = -1\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) \n",
      "\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "1950163739440 1950163739440\n"
     ]
    }
   ],
   "source": [
    "o_t = torch.tensor([1])\n",
    "print(o_t, '\\n')\n",
    "t = torch.zeros((3, 3), out=o_t)\n",
    "print(t, '\\n', o_t)\n",
    "print(id(t), id(o_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.zeros_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1., -1.], [1., -1.]])\n",
    "t2 = torch.zeros_like(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.ones/torch.ones_like/torch.full/torch.full_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 3.1416, 3.1416],\n",
      "        [3.1416, 3.1416, 3.1416]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.full((2, 3), 3.141592))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.arange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.5000, 2.0000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "# 创建等差的1维张量，长度为 (end-start)/step，需要注意数值区间为[start, end)。\n",
    "# 主要参数：\n",
    "# start (Number) – 数列起始值，默认值为0。the starting value for the set of points. Default: 0.\n",
    "# end (Number) – 数列的结束值。\n",
    "# step (Number) – 数列的等差值，默认值为1。\n",
    "# out (Tensor, optional) – 输出的tensor，即该函数返回的tensor可以通过out进行赋值。\n",
    "print(torch.arange(1, 2.51, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.linspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])\n",
      "tensor([1., 3., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 创建均分的1维张量，长度为steps，区间为[start, end]。\n",
    "# 主要参数：\n",
    "# start (float) – 数列起始值\n",
    "# end (float) – 数列结束值。\n",
    "# steps (int) – 数列长度。\n",
    "print(torch.linspace(3, 10, steps=5))\n",
    "print(torch.linspace(1, 5, steps=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.logspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3250022521650377\n",
      "0.5499958515306512\n",
      "0.7749984371602154\n",
      "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# 创建对数均分的1维张量，长度为steps, 底为base。\n",
    "# 主要参数：\n",
    "# start (float) – 确定数列起始值为base^start\n",
    "# end (float) – 确定数列结束值为base^end\n",
    "# steps (int) – 数列长度。\n",
    "# base (float) - 对数函数的底，默认值为10，此参数是在pytorch 1.0.1版本之后加入的。\n",
    "print(np.log10(2.1135))\n",
    "print(np.log10(3.5481))\n",
    "print(np.log10(5.9566))\n",
    "print(torch.logspace(start=0.1, end=1.0, steps=5))\n",
    "print(torch.logspace(start=2, end=2, steps=1, base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.eye\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 创建单位对角矩阵。\n",
    "# 主要参数：\n",
    "# n (int) - 矩阵的行数\n",
    "# m (int, optional) - 矩阵的列数，默认值为n，即默认创建一个方阵\n",
    "print(np.eye(3))\n",
    "print(np.eye(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3190e+16, 1.2892e-42, 3.1416e+00],\n",
       "        [3.1416e+00, 3.1416e+00, 3.1416e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ：依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。\n",
    "# 主要参数：\n",
    "# size (int...) - 张量维度\n",
    "# pin_memory (bool, optional) - pinned memory 又称page locked memory，即锁页内存，该参数用来指示是否将tensor存\n",
    "# 于锁页内存，通常为False，若内存足够大，建议设置为True，这样在转到GPU时会快一些。\n",
    "torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty_like(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2111e+16, 1.2892e-42],\n",
       "        [5.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.empty_like之于torch.empty等同于torch.zeros_like之于torch.zeros，因此不再赘述。\n",
    "torch.empty_like(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty_strided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.3759e+16,  1.2892e-42,  0.0000e+00,  1.1250e-05],\n",
      "        [ 1.0842e-19,  2.1935e+00, -1.0842e-19,  2.3723e+00],\n",
      "        [ 0.0000e+00,  2.5625e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[ 0.0000e+00,  1.1250e-05, -1.0842e-19,  2.5625e+00],\n",
      "        [ 2.4026e+02,  1.0842e-19,  2.3723e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  2.1935e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[ 1.1250e-05,  2.5625e+00,  1.0842e-19],\n",
      "        [-1.0842e-19,  2.4026e+02,  2.3723e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。\n",
    "# 主要参数：\n",
    "# stride (tuple of python:ints) - 张量存储在内存中的步长，是设置在内存中的存储方式。\n",
    "# size (int...) - 张量维度\n",
    "# pin_memory (bool, optional) - 是否存于锁页内存\n",
    "\n",
    "# 这里 stride=(4,1)，表示：\n",
    "# 在第一维（行），每跨 1 步，需要跳 4 个元素。\n",
    "# 在第二维（列），每跨 1 步，需要跳 1 个元素。\n",
    "# 这等价于 torch.empty((3,4))，使用的是默认的 行优先（Row-major） 存储方式。\n",
    "# 创建一个 (3, 4) 形状的张量，步幅 (4, 1)，即默认的连续内存布局\n",
    "tensor = torch.empty_strided((3, 4), (4, 1))\n",
    "print(tensor)\n",
    "\n",
    "# 这里 stride=(1, 3)，表示：\n",
    "# 在第一维（行），每跨 1 步，只跳 1 个元素（行数据变得不连续）。\n",
    "# 在第二维（列），每跨 1 步，需要跳 3 个元素。\n",
    "# 这会导致列变得不连续，因为通常 PyTorch 张量是行优先存储的，而这里的 stride 让数据以列优先的方式存储。\n",
    "# # 创建一个 (3, 4) 形状的张量，步幅 (1, 3)\n",
    "tensor = torch.empty_strided((3, 4), (1, 3))\n",
    "print(tensor)\n",
    "\n",
    "# 这里 stride=(1,2)，表示：\n",
    "# 在 第一维（行），每前进 1 步，跳 1 个元素。\n",
    "# 在 第二维（列），每前进 1 步，跳 2 个元素（表示列是主维度）。\n",
    "# 这个张量的 内存布局与其转置匹配，可以更高效地执行 tensor.t()。\n",
    "# 创建一个 (2, 3) 形状的张量，并让它的步幅与转置 (3, 2) 形状匹配\n",
    "tensor = torch.empty_strided((2, 3), (1, 2))  # 行步幅=1，列步幅=2\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), \n",
      "std: tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
      "        0.1000]), \n",
      "normal: tensor([ 1.1185,  2.0894,  3.5925,  4.4421,  4.8219,  5.0937,  7.3572,  8.4138,\n",
      "         8.9422, 10.0821])\n"
     ]
    }
   ],
   "source": [
    "# 为每一个元素以给定的mean和std用高斯分布生成随机数\n",
    "# 主要参数：\n",
    "# mean (Tensor or Float) - 高斯分布的均值，\n",
    "# std (Tensor or Float) - 高斯分布的标准差\n",
    "# 特别注意事项：\n",
    "# mean和std的取值分别有2种，共4种组合，不同组合产生的效果也不同，需要注意\n",
    "# mean为张量，std为张量，torch.normal(mean, std, out=None)，每个元素从不同的高斯分布采样，分布的均值和标准差\n",
    "# 由mean和std对应位置元素的值确定；\n",
    "# mean为张量，std为标量，torch.normal(mean, std=1.0, out=None)，每个元素采用相同的标准差，不同的均值；\n",
    "# mean为标量，std为张量，torch.normal(mean=0.0, std, out=None)， 每个元素采用相同均值，不同标准差；\n",
    "# mean为标量，std为标量，torch.normal(mean, std, size, *, out=None) ，从一个高斯分布中生成大小为size的张量\n",
    "mean = torch.arange(1, 11.)\n",
    "std = torch.arange(1, 0, -0.1)\n",
    "normal = torch.normal(mean=mean, std=std)\n",
    "# normal第一个元素是从均值为1，标准差为1的高斯分布中采样得到的\n",
    "# 第二个元素是从均值为2，标准差为0.9的高斯分布中采样得到的\n",
    "print(\"mean: {}, \\nstd: {}, \\nnormal: {}\".format(mean, std, normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
