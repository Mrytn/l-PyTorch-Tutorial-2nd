{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'conv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.function import Function\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量相关函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., -1.],\n",
      "        [ 1., -1.]]) torch.float32\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32) torch.int32\n"
     ]
    }
   ],
   "source": [
    "l = [[1., -1.], [1., -1.]]\n",
    "t_from_list = torch.tensor(l)\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t_from_array = torch.tensor(arr)\n",
    "print(t_from_list, t_from_list.dtype)\n",
    "print(t_from_array, t_from_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.from_numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array:  [[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor :  tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "\n",
      "修改arr\n",
      "numpy array:  [[0 2 3]\n",
      " [4 5 6]]\n",
      "tensor :  tensor([[0, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "\n",
      "修改tensor\n",
      "numpy array:  [[-1  2  3]\n",
      " [ 4  5  6]]\n",
      "tensor :  tensor([[-1,  2,  3],\n",
      "        [ 4,  5,  6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "t_from_numpy = torch.from_numpy(arr)\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)\n",
    "print(\"\\n修改arr\")\n",
    "arr[0, 0] = 0\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)\n",
    "print(\"\\n修改tensor\")\n",
    "t_from_numpy[0, 0] = -1\n",
    "print(\"numpy array: \", arr)\n",
    "print(\"tensor : \", t_from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) \n",
      "\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "1999468913648 1999468913648\n"
     ]
    }
   ],
   "source": [
    "o_t = torch.tensor([1])\n",
    "print(o_t, '\\n')\n",
    "t = torch.zeros((3, 3), out=o_t)\n",
    "print(t, '\\n', o_t)\n",
    "print(id(t), id(o_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.zeros_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1., -1.], [1., -1.]])\n",
    "t2 = torch.zeros_like(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.ones/torch.ones_like/torch.full/torch.full_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 3.1416, 3.1416],\n",
      "        [3.1416, 3.1416, 3.1416]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.full((2, 3), 3.141592))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.arange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.5000, 2.0000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "# 创建等差的1维张量，长度为 (end-start)/step，需要注意数值区间为[start, end)。\n",
    "# 主要参数：\n",
    "# start (Number) – 数列起始值，默认值为0。the starting value for the set of points. Default: 0.\n",
    "# end (Number) – 数列的结束值。\n",
    "# step (Number) – 数列的等差值，默认值为1。\n",
    "# out (Tensor, optional) – 输出的tensor，即该函数返回的tensor可以通过out进行赋值。\n",
    "print(torch.arange(1, 2.51, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.linspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])\n",
      "tensor([1., 3., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 创建均分的1维张量，长度为steps，区间为[start, end]。\n",
    "# 主要参数：\n",
    "# start (float) – 数列起始值\n",
    "# end (float) – 数列结束值。\n",
    "# steps (int) – 数列长度。\n",
    "print(torch.linspace(3, 10, steps=5))\n",
    "print(torch.linspace(1, 5, steps=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.logspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3250022521650377\n",
      "0.5499958515306512\n",
      "0.7749984371602154\n",
      "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# 创建对数均分的1维张量，长度为steps, 底为base。\n",
    "# 主要参数：\n",
    "# start (float) – 确定数列起始值为base^start\n",
    "# end (float) – 确定数列结束值为base^end\n",
    "# steps (int) – 数列长度。\n",
    "# base (float) - 对数函数的底，默认值为10，此参数是在pytorch 1.0.1版本之后加入的。\n",
    "print(np.log10(2.1135))\n",
    "print(np.log10(3.5481))\n",
    "print(np.log10(5.9566))\n",
    "print(torch.logspace(start=0.1, end=1.0, steps=5))\n",
    "print(torch.logspace(start=2, end=2, steps=1, base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.eye\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 创建单位对角矩阵。\n",
    "# 主要参数：\n",
    "# n (int) - 矩阵的行数\n",
    "# m (int, optional) - 矩阵的列数，默认值为n，即默认创建一个方阵\n",
    "print(np.eye(3))\n",
    "print(np.eye(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8159e+16, 1.1210e-42, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ：依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。\n",
    "# 主要参数：\n",
    "# size (int...) - 张量维度\n",
    "# pin_memory (bool, optional) - pinned memory 又称page locked memory，即锁页内存，该参数用来指示是否将tensor存\n",
    "# 于锁页内存，通常为False，若内存足够大，建议设置为True，这样在转到GPU时会快一些。\n",
    "torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty_like(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8029e+16, 1.1210e-42],\n",
       "        [5.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.empty_like之于torch.empty等同于torch.zeros_like之于torch.zeros，因此不再赘述。\n",
    "torch.empty_like(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.empty_strided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 依size创建“空”张量，这里的“空”指的是不会进行初始化赋值操作。\n",
    "# 主要参数：\n",
    "# stride (tuple of python:ints) - 张量存储在内存中的步长，是设置在内存中的存储方式。\n",
    "# size (int...) - 张量维度\n",
    "# pin_memory (bool, optional) - 是否存于锁页内存\n",
    "\n",
    "# 这里 stride=(4,1)，表示：\n",
    "# 在第一维（行），每跨 1 步，需要跳 4 个元素。\n",
    "# 在第二维（列），每跨 1 步，需要跳 1 个元素。\n",
    "# 这等价于 torch.empty((3,4))，使用的是默认的 行优先（Row-major） 存储方式。\n",
    "# 创建一个 (3, 4) 形状的张量，步幅 (4, 1)，即默认的连续内存布局\n",
    "tensor = torch.empty_strided((3, 4), (4, 1))\n",
    "print(tensor)\n",
    "\n",
    "# 这里 stride=(1, 3)，表示：\n",
    "# 在第一维（行），每跨 1 步，只跳 1 个元素（行数据变得不连续）。\n",
    "# 在第二维（列），每跨 1 步，需要跳 3 个元素。\n",
    "# 这会导致列变得不连续，因为通常 PyTorch 张量是行优先存储的，而这里的 stride 让数据以列优先的方式存储。\n",
    "# # 创建一个 (3, 4) 形状的张量，步幅 (1, 3)\n",
    "tensor = torch.empty_strided((3, 4), (1, 3))\n",
    "print(tensor)\n",
    "\n",
    "# 这里 stride=(1,2)，表示：\n",
    "# 在 第一维（行），每前进 1 步，跳 1 个元素。\n",
    "# 在 第二维（列），每前进 1 步，跳 2 个元素（表示列是主维度）。\n",
    "# 这个张量的 内存布局与其转置匹配，可以更高效地执行 tensor.t()。\n",
    "# 创建一个 (2, 3) 形状的张量，并让它的步幅与转置 (3, 2) 形状匹配\n",
    "tensor = torch.empty_strided((2, 3), (1, 2))  # 行步幅=1，列步幅=2\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), \n",
      "std: tensor([1.0000, 0.9000, 0.8000, 0.7000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
      "        0.1000]), \n",
      "normal: tensor([ 1.7729,  2.2683,  1.8957,  4.1742,  4.7767,  6.4824,  6.7675,  8.6944,\n",
      "         8.7786, 10.0303])\n"
     ]
    }
   ],
   "source": [
    "# 为每一个元素以给定的mean和std用高斯分布生成随机数\n",
    "# 主要参数：\n",
    "# mean (Tensor or Float) - 高斯分布的均值，\n",
    "# std (Tensor or Float) - 高斯分布的标准差\n",
    "# 特别注意事项：\n",
    "# mean和std的取值分别有2种，共4种组合，不同组合产生的效果也不同，需要注意\n",
    "# mean为张量，std为张量，torch.normal(mean, std, out=None)，每个元素从不同的高斯分布采样，分布的均值和标准差\n",
    "# 由mean和std对应位置元素的值确定；\n",
    "# mean为张量，std为标量，torch.normal(mean, std=1.0, out=None)，每个元素采用相同的标准差，不同的均值；\n",
    "# mean为标量，std为张量，torch.normal(mean=0.0, std, out=None)， 每个元素采用相同均值，不同标准差；\n",
    "# mean为标量，std为标量，torch.normal(mean, std, size, *, out=None) ，从一个高斯分布中生成大小为size的张量\n",
    "mean = torch.arange(1, 11.)\n",
    "std = torch.arange(1, 0, -0.1)\n",
    "normal = torch.normal(mean=mean, std=std)\n",
    "# normal第一个元素是从均值为1，标准差为1的高斯分布中采样得到的\n",
    "# 第二个元素是从均值为2，标准差为0.9的高斯分布中采样得到的\n",
    "print(\"mean: {}, \\nstd: {}, \\nnormal: {}\".format(mean, std, normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.rand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8808, 0.3411, 0.3451],\n",
       "        [0.9658, 0.4336, 0.2045]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在区间[0, 1)上，生成均匀分布。\n",
    "torch.rand(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3],\n",
       "        [3, 4, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在区间[low, high)上，生成整数的均匀分布。\n",
    "torch.randint(3, 5, (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.randint_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.randint_like之于torch.randint等同于torch.zeros_like之于torch.zeros，\n",
    "torch.randint_like(t1, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.randn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8908,  0.6130, -0.9899],\n",
       "        [-0.5479,  1.1192, -0.3060]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成形状为size的标准正态分布张量。\n",
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.randn_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8675, -0.4630],\n",
       "        [ 0.3928,  0.7077]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.randperm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 4, 2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成从0到n-1的随机排列。perm == permutation\n",
    "torch.randperm(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability: \n",
      "tensor([[0.8292, 0.3662, 0.0520],\n",
      "        [0.2720, 0.6393, 0.6570],\n",
      "        [0.5829, 0.5551, 0.7389]]), \n",
      "bernoulli_tensor:\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 以input的值为概率，生成伯努力分布（0-1分布，两点分布）。\n",
    "# # 生成一个 3×3 的张量，每个元素在 [0,1) 之间均匀分布\n",
    "p = torch.empty(3, 3).uniform_(0, 1)\n",
    "# 以 p 的值作为成功概率，生成伯努利分布（0 或 1）\n",
    "b = torch.bernoulli(p)\n",
    "print(\"probability: \\n{}, \\nbernoulli_tensor:\\n{}\".format(p, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.gather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([40, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个 1 维张量\n",
    "input = torch.tensor([10, 20, 30, 40, 50])\n",
    "# 定义索引张量\n",
    "index = torch.tensor([3, 0, 4])\n",
    "# 在 dim=0 维度上收集数据\n",
    "output = torch.gather(input, dim=0, index=index)\n",
    "print(output)  # 输出: tensor([40, 10, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70, 50, 30],\n",
      "        [10, 80, 60],\n",
      "        [40, 20, 90]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[10, 20, 30],\n",
    "                      [40, 50, 60],\n",
    "                      [70, 80, 90]])\n",
    "index = torch.tensor([[2, 1, 0],\n",
    "                      [0, 2, 1],\n",
    "                      [1, 0, 2]])\n",
    "output = torch.gather(input, dim=0, index=index)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30, 10, 20],\n",
      "        [50, 60, 40],\n",
      "        [70, 80, 90]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[10, 20, 30],\n",
    "                      [40, 50, 60],\n",
    "                      [70, 80, 90]])\n",
    "index = torch.tensor([[2, 0, 1],\n",
    "                      [1, 2, 0],\n",
    "                      [0, 1, 2]])\n",
    "output = torch.gather(input, dim=1, index=index)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 scores:\n",
      " tensor([[2.5000, 1.8000],\n",
      "        [3.1000, 2.0000],\n",
      "        [3.5000, 2.1000]])\n",
      "Top-2 indices:\n",
      " tensor([[1, 3],\n",
      "        [2, 3],\n",
      "        [1, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 假设是一个 3 样本 × 4 类的 logits（未归一化概率）\n",
    "logits = torch.tensor([[0.1, 2.5, 0.3, 1.8],\n",
    "                       [1.2, 0.4, 3.1, 2.0],\n",
    "                       [2.1, 3.5, 0.5, 0.7]])\n",
    "\n",
    "# 获取每个样本 Top-2 的索引和分数\n",
    "top2_scores, top2_indices = torch.topk(logits, k=2, dim=1)\n",
    "\n",
    "print(\"Top-2 scores:\\n\", top2_scores)\n",
    "print(\"Top-2 indices:\\n\", top2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1000, 2.5000, 2.8000, 3.0000, 2.3000],\n",
      "        [2.3000, 2.1000, 2.6000, 3.2000, 2.7000]])\n"
     ]
    }
   ],
   "source": [
    "# 假设 batch_size=2, seq_len=5, vocab_size=4\n",
    "logits = torch.tensor([[[0.1, 0.3, 2.1, 1.7],\n",
    "                        [1.2, 2.5, 0.4, 1.9],\n",
    "                        [1.3, 1.1, 2.8, 0.7],\n",
    "                        [2.2, 0.6, 1.4, 3.0],\n",
    "                        [0.5, 2.3, 1.7, 0.9]],\n",
    "                       [[1.1, 2.3, 0.8, 1.4],\n",
    "                        [0.6, 1.7, 2.1, 1.2],\n",
    "                        [1.0, 0.5, 2.6, 1.3],\n",
    "                        [0.4, 3.2, 1.5, 2.0],\n",
    "                        [1.8, 1.1, 0.9, 2.7]]])\n",
    "# 目标索引（假设真实的词索引）\n",
    "target_indices = torch.tensor([[2, 1, 2, 3, 1],\n",
    "                               [1, 2, 2, 1, 3]])\n",
    "# 获取 target_indices 对应的 logits\n",
    "target_logits = torch.gather(\n",
    "    logits, dim=2, index=target_indices.unsqueeze(-1)).squeeze(-1)\n",
    "print(target_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 80., 60.],\n",
      "        [40., 20., 90.],\n",
      "        [70., 50., 30.]])\n"
     ]
    }
   ],
   "source": [
    "# scatter_(dim, index, src, reduce=None) → Tensor。将src中数据根据index中的索引按照dim的方向\n",
    "# 填进input中。这是一个十分难理解的函数，其中index是告诉你哪些位置需要变，src是告诉你要变\n",
    "# 的值是什么。\n",
    "input_tensor = torch.zeros(3, 3)\n",
    "index = torch.tensor([[0, 1, 2],\n",
    "                      [1, 2, 0],\n",
    "                      [2, 0, 1]])\n",
    "src = torch.tensor([[10, 20, 30],\n",
    "                    [40, 50, 60],\n",
    "                    [70, 80, 90]])\n",
    "src = src.to(input_tensor.dtype)  # 转换 src 数据类型\n",
    "# 在 dim=0（行方向）上 scatter\n",
    "result = input_tensor.scatter(dim=0, index=index, src=src)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "is_leaf:\n",
      " True True False False False\n",
      "gradient:\n",
      " tensor([5.]) tensor([2.]) None None None\n",
      "grad_fn:\n",
      " None None <AddBackward0 object at 0x000001D189B75A20> <AddBackward0 object at 0x000001D189B74A00> <MulBackward0 object at 0x000001D189B75390>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hankuke.D\\AppData\\Local\\Temp\\ipykernel_22612\\3464209613.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"gradient:\\n\", w.grad, x.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)  # retain_grad()\n",
    "y = torch.mul(a, b)\n",
    "y.backward()\n",
    "print(w.grad)\n",
    "# 查看叶子结点\n",
    "print(\"is_leaf:\\n\", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "# 查看梯度\n",
    "print(\"gradient:\\n\", w.grad, x.grad, a.grad, b.grad, y.grad)\n",
    "# 查看 grad_fn\n",
    "print(\"grad_fn:\\n\", w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "y.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(w\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m----> 9\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(w\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32md:\\miniforge3\\envs\\v3.11.6\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniforge3\\envs\\v3.11.6\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniforge3\\envs\\v3.11.6\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 报错\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "y.backward()\n",
    "print(w.grad)\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y0 = torch.mul(a, b)  # y0 = (x+w) * (w+1) dy0/dw = 2w + x + 1\n",
    "y1 = torch.add(a, b)  # y1 = (x+w) + (w+1) dy1/dw = 2\n",
    "loss = torch.cat([y0, y1], dim=0)  # [y0, y1]\n",
    "# rad_tensors=[1., 2.] 作用是加权求梯度，决定 y0 和 y1 在最终梯度中的贡献。\n",
    "# 若 grad_tensors=[1., 1.]，则所有 loss 元素等权重贡献。\n",
    "# 你的 grad_tensors=[1., 2.] 代表 y1 的贡献是 y0 的两倍。\n",
    "grad_tensors = torch.tensor([1., 2.])\n",
    "# Tensor.backward中的 gradient 传入 torch.autograd.backward()中的grad_tensors\n",
    "loss.backward(gradient=grad_tensors)\n",
    "# w = 1* (dy0/dw) + 2*(dy1/dw)\n",
    "# w = 1* (2w + x + 1) + 2*(w)\n",
    "# w = 1* (5) + 2*(2)\n",
    "# w = 9\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "# 计算outputs对inputs的导数\n",
    "# 主要参数：\n",
    "# outputs (sequence of Tensor) – 用于求导的张量，如loss\n",
    "# inputs (sequence of Tensor) – 所要计算导数的张量\n",
    "# grad_outputs (sequence of Tensor) – 雅克比向量积中使用。\n",
    "# retain_graph (bool, optional) – 是否需要保留计算图。pytorch的机制是在方向传播结束时，计算图释放以节省内\n",
    "# 存。大家可以尝试连续使用loss.backward()，就会报错。如果需要多次求导，则在执行backward()时，\n",
    "# retain_graph=True。\n",
    "# create_graph (bool, optional) – 是否创建计算图，用于高阶求导。\n",
    "# allow_unused (bool, optional) – 是否需要指示，计算梯度时未使用的张量是错误的\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y = torch.pow(x, 2)  # y = x**2\n",
    "# 一阶导数\n",
    "# grad_1 = dy/dx = 2x = 2 * 3 = 6\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph=True)\n",
    "print(grad_1)\n",
    "# 二阶导数\n",
    "# grad_2 = d(dy/dx)/dx = d(2x)/dx = 2\n",
    "grad_2 = torch.autograd.grad(grad_1[0], x)\n",
    "print(grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.autograd.Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "# 有的时候，想要实现自己的一些操作（op），如特殊的数学函数、pytorch的module中没有的网络层，那就需要自己写\n",
    "# 一个Function，在Function中定义好forward的计算公式、backward的计算公式，然后将这些op组合到模型中，模型就可\n",
    "# 以用autograd完成梯度求取。\n",
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        result = i.exp()\n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        # ============== step2: 结果保存，用于反向传播 ==============\n",
    "        ctx.save_for_backward(result)\n",
    "        # ============== step2: 结果保存，用于反向传播 ==============\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # ============== step1: 取出结果，用于反向传播 ==============\n",
    "        result, = ctx.saved_tensors\n",
    "        # ============== step1: 取出结果，用于反向传播 ==============\n",
    "        # ============== step2: 反向传播公式实现 ==============\n",
    "        grad_results = grad_output * result\n",
    "        # ============== step2: 反向传播公式实现 ==============\n",
    "        return grad_results\n",
    "\n",
    "\n",
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = Exp.apply(x)  # 需要使用apply方法调用自定义autograd function\n",
    "print(y)  # y = e^x = e^1 = 2.7183\n",
    "y.backward()\n",
    "print(x.grad)  # 反传梯度, x.grad = dy/dx = e^x = e^1 = 2.7183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradcoeff\n",
    "\n",
    "反向传梯度时乘以一个自定义系数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], grad_fn=<PowBackward0>)\n",
      "tensor([-0.4000])\n"
     ]
    }
   ],
   "source": [
    "class GradCoeff(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, coeff):\n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        ctx.coeff = coeff  # 将coeff存为ctx的成员变量\n",
    "        x.view_as(x)  # 这个语句没有实际作用，可以删除\n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        # x=x ** 2\n",
    "        return x  # 直接返回 x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # backward的输出个数应与 forward 的输入个数相同\n",
    "        # 此处 coeff 不需要梯度，因此返回 None\n",
    "        return ctx.coeff * grad_output, None  # 仅 x 需要梯度，coeff 不需要梯度\n",
    "\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "ret = GradCoeff.apply(x, -0.1)  # 前向传播不会影响 x，仅存储 coeff = -0.1\n",
    "ret = ret ** 2  # ret = x^2\n",
    "print(ret)  # 观察 ret.grad_fn\n",
    "ret.backward()\n",
    "print(x.grad)  # 观察 x 的梯度\n",
    "# tensor([4.])：这是 ret = x^2 的前向计算结果。\n",
    "# tensor([-0.4])：是 x 的梯度，说明 coeff 确实在反向传播时起作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勒让德多项式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0\n"
     ]
    }
   ],
   "source": [
    "class LegendrePolynomial3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        y = 0.5 * (5 * x ** 3 - 3 * x)\n",
    "        ctx.save_for_backward(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        ret, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * ret ** 2 - 1)\n",
    "\n",
    "\n",
    "a, b, c, d = 1, 2, 1, 2\n",
    "x = 1\n",
    "P3 = LegendrePolynomial3.apply\n",
    "y_pred = a + b * P3(c + d * x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现 2D 卷积\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward: None\n",
      "After backward: tensor([[[[1.2584, 1.3009, 1.2439],\n",
      "          [1.2616, 1.2758, 1.1893],\n",
      "          [1.2007, 1.2413, 1.1897]],\n",
      "\n",
      "         [[1.1759, 1.2055, 1.2032],\n",
      "          [1.1251, 1.1885, 1.2216],\n",
      "          [1.1865, 1.2187, 1.1936]],\n",
      "\n",
      "         [[1.1953, 1.2126, 1.1893],\n",
      "          [1.2210, 1.2172, 1.2267],\n",
      "          [1.3208, 1.2731, 1.2899]]],\n",
      "\n",
      "\n",
      "        [[[1.4673, 1.5456, 1.4312],\n",
      "          [1.4455, 1.4835, 1.4049],\n",
      "          [1.3941, 1.4593, 1.3443]],\n",
      "\n",
      "         [[1.3166, 1.4030, 1.4102],\n",
      "          [1.3215, 1.3903, 1.3953],\n",
      "          [1.3581, 1.4129, 1.4145]],\n",
      "\n",
      "         [[1.3849, 1.4062, 1.4063],\n",
      "          [1.4378, 1.4148, 1.4395],\n",
      "          [1.4994, 1.4943, 1.4856]]],\n",
      "\n",
      "\n",
      "        [[[1.5341, 1.6044, 1.5186],\n",
      "          [1.5154, 1.5646, 1.4659],\n",
      "          [1.4746, 1.5180, 1.4588]],\n",
      "\n",
      "         [[1.3969, 1.4951, 1.4863],\n",
      "          [1.3774, 1.4612, 1.4730],\n",
      "          [1.4444, 1.4884, 1.5019]],\n",
      "\n",
      "         [[1.4678, 1.4716, 1.4589],\n",
      "          [1.5093, 1.5251, 1.5028],\n",
      "          [1.5839, 1.5708, 1.5479]]],\n",
      "\n",
      "\n",
      "        [[[1.2669, 1.3109, 1.2312],\n",
      "          [1.2616, 1.2776, 1.2086],\n",
      "          [1.2022, 1.2587, 1.1737]],\n",
      "\n",
      "         [[1.1615, 1.1975, 1.2360],\n",
      "          [1.1298, 1.2053, 1.2236],\n",
      "          [1.1928, 1.2337, 1.2137]],\n",
      "\n",
      "         [[1.2230, 1.2170, 1.2212],\n",
      "          [1.2494, 1.2179, 1.2699],\n",
      "          [1.2998, 1.3048, 1.3081]]],\n",
      "\n",
      "\n",
      "        [[[1.4758, 1.5593, 1.4595],\n",
      "          [1.4783, 1.5154, 1.4067],\n",
      "          [1.4091, 1.4778, 1.3833]],\n",
      "\n",
      "         [[1.3696, 1.4126, 1.4299],\n",
      "          [1.3268, 1.3834, 1.4061],\n",
      "          [1.3941, 1.4320, 1.4312]],\n",
      "\n",
      "         [[1.4344, 1.4127, 1.4220],\n",
      "          [1.4592, 1.4429, 1.4823],\n",
      "          [1.5458, 1.5162, 1.5248]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def convolution_backward(grad_out, X, weight):\n",
    "    \"\"\" 反向传播函数，返回与 forward 输入参数数目一致的梯度 \"\"\"\n",
    "    grad_X = F.conv_transpose2d(grad_out, weight)  # 计算 X 的梯度\n",
    "    grad_weight = F.conv2d(X.transpose(0, 1), grad_out.transpose(\n",
    "        0, 1)).transpose(0, 1)  # 计算 weight 的梯度\n",
    "    return grad_X, grad_weight\n",
    "\n",
    "\n",
    "class MyConv2D(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        ctx.save_for_backward(X, weight)\n",
    "        return F.conv2d(X, weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        X, weight = ctx.saved_tensors\n",
    "        return convolution_backward(grad_out, X, weight)\n",
    "\n",
    "\n",
    "# 测试\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\n",
    "# 检查自定义的 Autograd Function 是否正确\n",
    "# torch.autograd.gradcheck(MyConv2D.apply, (X, weight)) 在调用正向传播 y = MyConv2D.apply(X, weight) 之前的原因 是因为 gradcheck 本质上会自己执行 forward 和 backward，并进行数值梯度验证。\n",
    "torch.autograd.gradcheck(MyConv2D.apply, (X, weight))\n",
    "y = MyConv2D.apply(X, weight)\n",
    "label = torch.randn_like(y)\n",
    "loss = F.mse_loss(y, label)\n",
    "print(\"Before backward:\", weight.grad)\n",
    "loss.backward()\n",
    "print(\"After backward:\", weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([10.])\n",
      "tensor([15.])\n",
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "# 知识点一：梯度不会自动清零\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "for i in range(4):\n",
    "    a = torch.add(w, x)\n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "    y.backward()\n",
    "    print(w.grad)  # 梯度不会自动清零，数据会累加， 通常需要采用 optimizer.zero_grad() 完成对参数的梯度清零\n",
    "# w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "# 知识点二：依赖于叶子结点的结点，requires_grad默认为True\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "print(a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(a.is_leaf, b.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1943963332688 tensor([1.])\n",
      "1943963333360 tensor([2.])\n",
      "1943963333360 tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "# 知识点三：叶子张量不可以执行in-place操作\n",
    "a = torch.ones((1, ))\n",
    "print(id(a), a)\n",
    "a = a + torch.ones((1, ))\n",
    "print(id(a), a)\n",
    "a += torch.ones((1, ))\n",
    "print(id(a), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(w, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(a, b)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "w.add_(1)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([999.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 知识点四：detach 的作用\n",
    "# detach的作用是：从计算图中剥离出“数据”，并以一个新张量的形式返回，并且新张量与旧张量共享数据，简单的可理\n",
    "# 解为做了一个别名。\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "y.backward()\n",
    "w_detach = w.detach()\n",
    "w_detach.data[0] = 999\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识点五：with torch.no_grad()的作用\n",
    "# autograd自动构建计算图过程中会保存一系列中间变量，以便于backward的计算，这就必然需要花费额外的内存和时\n",
    "# 间。\n",
    "# 而并不是所有情况下都需要backward，例如推理的时候，因此可以采用上下文管理器——torch.no_grad()来管理上下\n",
    "# 文，让pytorch不记录相应的变量，以加快速度和节省空间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
